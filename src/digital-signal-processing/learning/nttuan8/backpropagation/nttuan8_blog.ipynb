{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Backpropagation",
   "id": "8a3e943c316a46e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Giờ ta cần tìm hệ số $W$ và $b$.",
   "id": "7e2b600a81d10635"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Có thể nghĩ ngay tới thuật toán **gradient descent** và việc quan trọng nhất trong thuật toán gradient descent là đi tìm đạo hàm của các hệ số đối với loss function. Bài này sẽ tính đạo hàm của các hệ số trong neural network với thuật toán **backpropagation**.",
   "id": "be1346cf82ccade4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bài toán XOR với Neural Network",
   "id": "c8f75ec3a8b1ca9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$x_{1}$ | $x_{2}$ | $x_{1} XOR x_{2}$\n",
    ":-|:-------:|-:\n",
    "1|    1    |0\n",
    "1|    0    |1\n",
    "0|1|1\n",
    "0|0|0"
   ],
   "id": "ba63af989cb79f8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model:",
   "id": "f3e6a2c9a11d980c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image1.png)",
   "id": "35a65def58263b6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Mô hình trên có 2 layers (số lượng layer của mô hình không tính input layer)\n",
    "- Mô hình 2-2-1, nghĩa là 2 node trong input layer, 1 hidden layer có 2 node và output layer có 1 node.\n",
    "- Input layer và hidden layer luôn thêm node 1 để tính bias cho layer sau, nhưng không tính vào số lượng node trong layer.\n",
    "- Ở mỗi node trong trong hidden layer và output layer đều thực hiện 2 bước: tính tổng linear + áp dụng activation function.\n",
    "- Các hệ số và bias tương ứng được ký hiệu như trong hình."
   ],
   "id": "45914fc1f1629201"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feedforward",
   "id": "8a2db3be882e7e2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- $z_{1}^{(1)} = b_{1}^{(1)} + x_{1}.w_{11}^{(1)} + x_{2}.w_{21}^{(1)}$\n",
    "- $a_{1}^{(1)} = \\sigma(z_{1}^{(1)})$\n",
    "- $z_{2}^{(1)} = b_{2}^{(1)} + x_{1}.w_{12}^{(1)} + x_{2}.w_{22}^{(1)}$\n",
    "- $a_{1}^{(1)} = \\sigma(z_{1}^{(1)})$\n",
    "- $z_{1}^{(2)} = b_{1}^{(2)} + a_{1}^{(1)}.w_{11}^{(2)} + a_{2}^{(1)}.w_{21}^{(2)}$\n",
    "- $\\hat{y} = a_{1}^{(2)} = \\sigma(z_{1}^{(2)})$"
   ],
   "id": "23ab5bf5816b7431"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Viết dưới dạng matrix:",
   "id": "d93b2c94f831e46b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "X = \\begin{bmatrix} 1&1\\\\1&0\\\\0&1\\\\0&0 \\end{bmatrix}, Y = \\begin{bmatrix} 0\\\\1\\\\1\\\\0 \\end{bmatrix}\n",
    "$$"
   ],
   "id": "63e857bd3778544"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$Z^{(1)} = X.W^{(1)} + b^{(1)}$\n",
    "\n",
    "$A^{(1)} = \\sigma(Z^{(1)})$\n",
    "\n",
    "$Z^{(2)} = A^{(1)}.W^{(2)} + b^{(2)}$\n",
    "\n",
    "$\\hat{Y} = A^{(2)} = \\sigma(Z^{(2)})$"
   ],
   "id": "d114fc94cf085258"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loss Function",
   "id": "8ccdabb06001e90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Với mỗi điểm $(x^{[i]}, y_{i})$, gọi hàm **loss function**:\n",
    "$$\n",
    "L = -(y_{i}.log(\\hat{y_{i}})+(1-y_{i}).log(1-\\hat{y_{i}}))\n",
    "$$\n",
    "\n",
    "Hàm **loss function** cho toàn bộ dữ liệu:\n",
    "$$\n",
    "J = -\\sum_{i=1}^{N}{(y_{i}.log(\\hat{y_{i}})+(1-y_{i}).log(1-\\hat{y_{i}}))}\n",
    "$$"
   ],
   "id": "b26773fc95edfbc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gradient Descent",
   "id": "7ebe402fae8fd52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Để áp dụng gradient descent, ta cần tính được đạo hàm của các hệ số $W$ và bias $b$ với hàm Loss Function.",
   "id": "316122c5bca0499"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Khi hàm $f(x)$ là hàm 1 biến $x$. Đạo hàm của $f$ đối với biến $x$ ký hiệu:\n",
    "$$\n",
    "\\frac{df}{dx}\n",
    "$$\n",
    "- Khi hàm $f(x,y)$ là hàm nhiều biến. Đạo hàm $f$ với biến $x$ kí hiệu là:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x}\n",
    "$$"
   ],
   "id": "c62324d01dc0052e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Với mỗi điểm $(x^{([i])}, y_{i})$, hàm **Loss Function**:\n",
    "$$\n",
    "L = -(y_{i}.log(\\hat{y_{i}}) + (1-y_{i}).log(1-\\hat{y_{i}}))\n",
    "$$\n",
    "\n",
    "- trong đó:\n",
    "    - $\\hat{y_{i}}$: là $a_{1}^{(2)} = \\sigma(a_{1}^{(1)}.w_{11}^{(2)}+a_{2}^{(2)}.w_{21}^{(2)} + b_{1}^{(2)})$.\n",
    "    - $y_{i}$: là giá trị thật của dữ liệu."
   ],
   "id": "5edb80188350cf26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Đạo hàm của **Loss Function**:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_{i}}}=\n",
    "-\\frac{\\partial(y_{i}.log(\\hat{y_{i}})+(1-y_{i}).log(1-\\hat{y_{i}}))}{\\partial \\hat{y_{i}}} =\n",
    "-(\\frac{y_{i}}{\\hat{y_{i}}}-\\frac{1-y_{i}}{(1-\\hat{y_{i}})})\n",
    "$$"
   ],
   "id": "f712351d8f2dafb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tính đạo hàm $L$ với $W^{(2)}, b^{(2)}$",
   "id": "61a254ef55b56c40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Áp dụng **Chain Rule** ta có:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_{1}^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y_{i}}}.\\frac{\\partial \\hat{y_{i}}}{\\partial b_{1}^{(2)}}\n",
    "$$"
   ],
   "id": "535ad7a4a19e725b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lý do sử dụng Chain Rule:",
   "id": "f53a4ae73f96d5cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Ta có Loss: $L=L(\\hat{y_{i}},y_{i})$\n",
    "    - và $\\hat{y_{i}}$ lại phụ thuộc vào $W$ và $b$ ở layer cuối:\n",
    "        $$\n",
    "        \\hat{y_{i}}=f(z^{(2)})=f(W^{(2)}.h^{(1)}+b^{(2)})\n",
    "        $$\n",
    "$\\rightarrow$ Tức là **Loss** không phụ thuộc trực tiếp vào $b^{(2)}$, mà phụ thuộc gián tiếp thông qua $\\hat{y_{i}}$."
   ],
   "id": "7937ab6b90590d55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image2.png)",
   "id": "7a361491d1db70fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Từ đồ thị ta thấy:\n",
    "$$\n",
    "\\frac{\\partial \\hat{y_{i}}}{\\partial b_{1}^{(2)}} = \\hat{y_{i}}.(1-\\hat{y_{i}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y_{i}}}{\\partial w_{11}^{(2)}} = a_{1}^{(1)}.\\hat{y_{i}}.(1-\\hat{y_{i}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y_{i}}}{\\partial w_{21}^{(2)}} = a_{2}^{(1)}.\\hat{y_{i}}.(1-\\hat{y_{i}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y_{i}}}{\\partial a_{1}^{(1)}} = w_{11}^{(2)}.\\hat{y_{i}}.(1-\\hat{y_{i}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y_{i}}}{\\partial a_{2}^{(1)}} = w_{21}^{(2)}.\\hat{y_{i}}.(1-\\hat{y_{i}})\n",
    "$$"
   ],
   "id": "207e422332d6e2a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Gradient** của **Loss Function** theo từng tham số ($b$,$w$,$a$)",
   "id": "680f233912212c71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$b$:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_{1}^{(2)}} =\n",
    "\\frac{\\partial L}{\\partial \\hat{y_{i}}}.\\frac{\\partial \\hat{y_{i}}}{\\partial b_{1}^{(2)}} =\n",
    "-(\\frac{y_{i}}{\\hat{y_{i}}}-\\frac{1-y_{i}}{(1-\\hat{y_{i}})}).\\hat{y_{i}}.(1-\\hat{y_{i}})\n",
    "$$\n",
    "$$\n",
    "=-(y_{i}.(1-\\hat{y_{i}})-(1-y_{i}).\\hat{y_{i}})\n",
    "$$\n",
    "$$\n",
    "= \\hat{y_{i}}-y_{i}\n",
    "$$"
   ],
   "id": "8dd0754df9838934"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$w$:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{11}^{(2)}} = a_{1}^{(1)}.(\\hat{y_{i}}-y_{i})\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{21}^{(2)}} = a_{2}^{(1)}.(\\hat{y_{i}}-y_{i})\n",
    "$$\n",
    "\n",
    "$a$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_{1}^{(1)}} = w_{11}^{(2)}.(\\hat{y_{i}}-y_{i})\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_{2}^{(1)}} = w_{21}^{(2)}.(\\hat{y_{i}}-y_{i})\n",
    "$$"
   ],
   "id": "289c3fae8ca4b74e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Biểu diễn dưới dạng ma trận",
   "id": "6405ccadb7f91c4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lưu ý:\n",
    "- Đạo hàm của $L$ có kích thước $m\\times n$ thì đạo hàm của $W$ cũng phải có kích thước là $n\\times m$."
   ],
   "id": "59542b96fd1cf6af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial W} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}}&...&\\frac{\\partial L}{\\partial w_{1n}}\\\\\n",
    "\\frac{\\partial L}{\\partial w_{21}}&...&\\frac{\\partial L}{\\partial w_{2n}}\\\\\n",
    "...&...&...\\\\\n",
    "\\frac{\\partial L}{\\partial w_{m1}}&...&\\frac{\\partial L}{\\partial w_{mn}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ],
   "id": "fd4fd65a6948773e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Gradient của một ma trận là một ma trận có cùng shape.\n",
    "- Mỗi phần tử là đạo hàm riêng của loss theo từng trọng số."
   ],
   "id": "5314f6ce4082f98c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$W^{(2)}$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(2)}} = (A^{(1)})^{T}.(\\hat{Y}-Y)\n",
    "$$\n",
    "\n",
    "$b^{(2)}$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b^{(2)}}=(\\sum(\\hat{Y}-Y))^{T}\n",
    "$$\n",
    "\n",
    "$A^{(1)}$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial A^{(1)}} = (W^{(2)})^T.(\\hat{Y}-Y)\n",
    "$$"
   ],
   "id": "dcacfa39bf47a36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vậy là đã tính xong đạo hàm của $L$ với hệ số $W^{(2)}, b^{(2)}$. Giờ sẽ tính đạo hàm của $L$ với hệ số $W^{(1)},b^{(1)}$.",
   "id": "afa55b15894a4ad8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Thế khi nào dùng element-wise $(\\otimes)$, khi nào dùng nhân ma trận (.)???",
   "id": "9113216a191e2ebf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Khi tính đạo hàm ngược lại qua bước activation thì dùng $(\\otimes)$\n",
    "- Khi có phép tính nhân ma trận thì dùng (.), nhưng đặc biệt chú ý đến **kích thước ma trận** và dùng **transpose** nếu cần thiết."
   ],
   "id": "effd691bcc90575b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image8.png)",
   "id": "f7e7ee6b5cfb1281"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vậy là đã tính xong hết đạo hàm của **Loss Function** với các hệ số $W$ và bias $b$, giờ có thể áp dụng **Gradient Descent** để giải bài toán.",
   "id": "5461d3704cdc61ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Ví dụ:** Giờ thử tính $\\frac{\\partial L}{\\partial x_{1}}$, ở bài này thì không cần vì chỉ có 1 hidden layer, nhưng nếu nhiều hơn 1 hidden layer thì bạn cần tính bước này để tính đạo hàm với các hệ số trước đó.",
   "id": "9142df047bb37088"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image3.png)",
   "id": "11fbbd9d5f680bda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ta thấy $w_{11}^{(1)}$ chỉ tác động đến $a_{1}^{(1)}$, cụ thể là $a_{1}^{(1)} = \\sigma(b_{1}^{(1)}) + x_{1}.w_{11}^{(1)} + x_{2}.w_{21}^{(1)}$",
   "id": "3f0e8deb4a7f6205"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tuy nhiên $x_{1}$ không những tác động đến $a_{1}^{(1)}$ mà còn tác động đến $a_{2}^{(1)}$, nên khi áp dụng **chain rule** tính đạo hàm của $L$ với $x_{1}$ cần tính tổng đạo hàm qua cả $a_{1}^{(1)}$ và $a_{2}^{(1)}$.",
   "id": "60806fabe908d228"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image4.png)",
   "id": "1cdd068827f08ad0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Do đó:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{1}} =\n",
    "\\frac{\\partial L}{\\partial a_{1}^{(1)}}.\\frac{\\partial a_{1}^{(1)}}{\\partial x_{1}} +\n",
    "\\frac{\\partial L}{\\partial a_{2}^{(1)}}.\\frac{\\partial a_{2}^{(1)}}{\\partial x_{1}}\n",
    "$$\n",
    "$$\n",
    "= w_{11}^{(1)}.a_{1}^{(1)}.(1-a_{1}^{(1)}).w_{11}^{(2)}.(y_{i}-\\hat{y_{i}}) +\n",
    "w_{12}^{(1)}.a_{2}^{(1)}.(1-a_{2}^{(1)}).w_{21}^{(2)}.(y_{i}-\\hat{y_{i}})\n",
    "$$"
   ],
   "id": "ade1cb1976cdad43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mô hình Tổng Quát",
   "id": "fd5135fc45053a30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image5.png)",
   "id": "a22df296d018ef22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- **Bước 1**: Tính $\\frac{\\partial J}{\\partial \\hat{Y}}$, trong đó $\\hat{Y}=A^{(3)}$",
   "id": "7ada622ca8e6cdea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- **Bước 2**: Tính $\\frac{\\partial J}{\\partial \\hat{W}^{(3)}}$,$\\frac{\\partial J}{\\partial \\hat{b}^{(3)}}$ và $\\frac{\\partial J}{\\partial \\hat{A}^{(2)}}$",
   "id": "7af2d9f564230931"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- **Bước 3**: Tính $\\frac{\\partial J}{\\partial \\hat{W}^{(2)}}$,$\\frac{\\partial J}{\\partial \\hat{b}^{(2)}}$ và $\\frac{\\partial J}{\\partial \\hat{A}^{(1)}}$",
   "id": "5126f5d85f512b38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- **Bước 4**: Tính $\\frac{\\partial J}{\\partial \\hat{W}^{(1)}}$,$\\frac{\\partial J}{\\partial \\hat{b}^{(1)}}$, trong đó $A^{(0)}=X$",
   "id": "4a29cee099faf7a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nếu network có nhiều layer hơn thì cứ tiếp tục cho đến khi tính được đạo hàm của loss function $J$ với tất cả các hệ số $W$ và bias $b$.",
   "id": "f3d180de4fda44a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FeedForward",
   "id": "b4a4ce6213b4afae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image6.png)",
   "id": "70fa51b9034fc2a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BackPropagation",
   "id": "8d9e01a21f9baad9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image7.png)",
   "id": "9fa8558448cd0cdf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Đấy là vì sao thuật toán được gọi là backpropagation (lan truyền ngược)",
   "id": "466f4231dd0e1e58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
