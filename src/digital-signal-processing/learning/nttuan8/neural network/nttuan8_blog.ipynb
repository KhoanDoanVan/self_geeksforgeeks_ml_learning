{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neural Network",
   "id": "b5116f29a9cf7ba1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Con chó có thể phân biệt được người thân trong gia đình và người lạ hay đứa trẻ có thể phân biệt được các con vật. Những việc tưởng chừng như rất đơn giản nhưng lại cực kì khó để thực hiện bằng máy tính. Vậy sự khác biệt nằm ở đâu? Câu trả lời nằm ở bộ não với lượng lớn các nơ-ron thần kinh liên kết với nhau. Thế thì máy tính có nên mô phỏng lại mô hình ấy để giải các bài toán trên ???\n",
    "\n",
    "Neural là tính từ của neuron (nơ-ron), network chỉ cấu trúc đồ thị nên neural network (NN) là một hệ thống tính toán lấy cảm hứng từ sự hoạt động của các nơ-ron trong hệ thần kinh."
   ],
   "id": "cc57d3ce52b90e9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nơ-ron là đơn vị cơ bản cấu tạo hệ thống thần kinh và là một phần quan trọng nhất của não. Não chúng ta gồm khoảng 10 triệu nơ-ron và mỗi nơ-ron liên kết với 10.000 nơ-ron khác.",
   "id": "44fb95f279410582"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ở mỗi nơ-ron có phần thân (soma) chứa nhân, các tín hiệu đầu vào qua sợi nhánh (dendrites) và các tín hiệu đầu ra qua sợi trục (axon) kết nối với các nơ-ron khác. Hiểu đơn giản mỗi nơ-ron nhận dữ liệu đầu vào qua sợi nhánh và truyền dữ liệu đầu ra qua sợi trục, đến các sợi nhánh của các nơ-ron khác.",
   "id": "8040ddfedc9996fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mỗi nơ-ron nhận xung điện từ các nơ-ron khác qua sợi nhánh. Nếu các xung điện này đủ lớn để kích hoạt nơ-ron, thì tín hiệu này đi qua sợi trục đến các sợi nhánh của các nơ-ron khác. => Ở mỗi nơ-ron cần quyết định có kích hoạt nơ-ron đấy hay không. Giống giống hàm sigmoid bài trước nhỉ ?",
   "id": "7d8c6a21c2473001"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tuy nhiên NN chỉ là lấy cảm hứng từ não bộ và cách nó hoạt động, chứ không phải bắt chước toàn bộ các chức năng của nó. Việc chính của chúng ta là dùng mô hình đấy đi giải quyết các bài toán chúng ta cần.",
   "id": "d9ad47bdf61a7191"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Mô hình Neural Network",
   "id": "f15aa7f976bdc8e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Logistic Regression",
   "id": "b69b85a3e3335b5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Logistic regression là mô hình neural network đơn giản nhất chỉ với input layer và output layer.",
   "id": "32c99b4bb3d81596"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Mô hình của logistic regression từ bài trước là:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w_{0}+w_{1}.x_{1}+w_{2}.x_{2})\n",
    "$$\n",
    "\n",
    "- 1. Tính tổng linear: $z = 1.w_{0} + x_{1}.w_{1} + x_{2}.w_{2}$\n",
    "- 2. Áp dụng sigmoid function: $\\hat{y}=\\sigma(z)$"
   ],
   "id": "c2402fc9b4335f8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image1.png)",
   "id": "cf84de43446f8ca3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Để biểu diễn gọn lại ta sẽ gồn hai bước trên thành một trên biểu đồ:",
   "id": "bd0ad3bf38a94bb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image2.png)",
   "id": "8b04b334a2c596a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hệ số $w_{0}$ được gọi là $bias$. Để ý từ những bài trước đến giờ dữ liệu khi tính toán được thêm **1** để tính hệ số bias $w_{0}$. Tại sao lại cần hệ số bias? Quay lại với bài 1, phương trình đường thẳng sẽ thế nào nếu bỏ $w_{0}$, phương trình giờ có dạng: $w_{1}.x + w_{2}.y = 0 \\rightarrow$ sẽ luôn đi qua gốc toạ độ và nó không tổng quát hoá phương trình đường thẳng nên có thể không tìm được phương trình mong muốn.",
   "id": "749e7583f1f365ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$\\rightarrow$ Việc thêm bias (hệ số tự do) là rất quan trọng.",
   "id": "5b04af66ad203fe6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hàm sigmoid ở đây được gọi là **activation function**.",
   "id": "cf43cec098096dd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Mô hình tổng quát",
   "id": "ad59fd08c70e00df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image3.png)",
   "id": "dd2cba26b49245c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Layer đầu tiên là input layer, các layer ở giữa được gọi là hidden layer, layer cuối cùng được gọi là output layer. Các hình tròn được gọi là node.",
   "id": "f09c0188bda7c02e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Mỗi mô hình luôn có 1 input layer, 1 output layer, có thể có hoặc không các hidden layer. Tổng số layer trong mô hình được quy ước là số layer – 1 (Không tính input layer).\n",
    "- Ví dụ như ở hình trên có 1 input layer, 2 hidden layer và 1 output layer. Số lượng layer của mô hình là 3 layer."
   ],
   "id": "83ea5228a0238734"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Mỗi node trong hidden layer và output layer :\n",
    "- Liên kết với tất cả các node ở layer trước đó với các hệ số w riêng.\n",
    "- Mỗi node có 1 hệ số bias b riêng.\n",
    "- Diễn ra 2 bước: tính tổng linear và áp dụng activation function."
   ],
   "id": "1d49554d8fcc249f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Kí hiệu",
   "id": "ff8041da1b46bfb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Số node trong hidden layer thứ $i$ là $l^{(i)}$.",
   "id": "8b85f5b14892ab85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ma trận $W^{(k)}$ kích thước $l^{(k-1)}.l^{(k)}$ là ma trận hệ số giữa layer $(k-1)$ và layer $k$, trong đó $w_{ij}^{(k)}$ là hệ số kết nối từ node thứ $i$ của layer $k-1$ đến node thứ $j$ của layer $k$.",
   "id": "8b7e343bc1bf21d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Với node thứ $i$ trong layer $l$ có bias $b_{i}^{(l)}$ thực hiện 2 bước:\n",
    "- Tính tổng linear:\n",
    "    $$\n",
    "    z_{i}^{(l)}=\\sum^{l^{(l-1)}}_{j=1}{a_{j}^{(l-1)}.w_{ij}^{(l)}+b_{i}^{(l)}}\n",
    "    $$\n",
    "  $\\rightarrow$ là tổng tất cả các node trong layer trước nhân với hệ số $w$ tương ứng, rồi cộng với bias $b$.\n",
    "\n",
    "- Áp dụng activation function:\n",
    "$$\n",
    "a_{i}^{(l)} = \\sigma(z_{i}^{(l)})\n",
    "$$"
   ],
   "id": "3fae483d3a4376e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vector $z^{(k)}$ kích thước $l^{(k)}.1$ là giá trị các node trong layer $k$ sau bước tính tổng linear.",
   "id": "f85190577e446868"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vector $a^{(k)}$ kích thước $l^{(k)}.1$ là giá trị của các node trong layer $k$ sau khi áp dụng activation function.",
   "id": "a493583ef5d67716"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image4.png)",
   "id": "4ee2265c45cb2f2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mô hình neural network trên gồm 3 layer. Input layer có 2 node $(l^{(0)}=2)$, hidden layer 1 có 3 node $(l^{(1)}=3)$, hidden layer 2 có 3 node $(l^{(2)}=3)$ và output layer có 1 node $(l^{(3)}=1)$.",
   "id": "14ded82e1c700bfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Do mỗi node trong hidden layer và output layer đều có bias nên trong input layer và hidden layer cần thêm node 1 để tính bias (nhưng không tính vào tổng số node layer có).",
   "id": "7d4a96b44e801e3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tại node thứ 2 ở layer 1, ta có:\n",
    "- $z_{2}^{(1)} = x_{1}.w_{12}^{(1)} + x_{2}.w_{22}^{(1)} + b_{2}^{(1)}$\n",
    "- $a_{2}^{(1)} = \\sigma(z_{2}^{(1)})$\n"
   ],
   "id": "3b0f697210a4f342"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tại node thứ 3 layer 2, ta có:\n",
    "- $z_{3}^{(2)} = a_{1}^{(1)}.w_{13}^{(2)} + a_{2}^{(1)}.w_{23}^{(2)} + a_{3}^{(1)}.w_{33}^{(2)} + b_{3}^{(2)}$\n",
    "- $a_{3}^{(2)} = \\sigma(z_{3}^{(2)})$"
   ],
   "id": "73d009df7fa9ee1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feedforward",
   "id": "c78d4c5f37b4d5d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Để nhất quán về mặt ký hiệu, gọi input layer là $a^{(0)}(=x)$ kích thước 2$\\times$1.",
   "id": "4683d68a80520680"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "z^{(1)} =\n",
    "\\begin{bmatrix}\n",
    "z_{1}^{(1)}\\\\z_{2}^{(1)}\\\\z_{3}^{(1)}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(0)}.w_{11}^{(1)} + a_{2}^{(0)}.w_{21}^{(1)} + b_{1}^{(1)}\\\\\n",
    "a_{1}^{(0)}.w_{12}^{(1)} + a_{2}^{(0)}.w_{22}^{(1)} + b_{2}^{(1)}\\\\\n",
    "a_{1}^{(0)}.w_{13}^{(1)} + a_{2}^{(0)}.w_{23}^{(1)} + b_{3}^{(1)}\\\\\n",
    "\\end{bmatrix} = (W^{(1)})^{T}.a^{(0)} + b^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(1)} = \\sigma(z^{(1)})\n",
    "$$"
   ],
   "id": "ff81f49f3027cbc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tương tự ta có:",
   "id": "7d7232f13e97ada6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- $z^{(2)}=(W^{(2)})^{T}.a^{(1)} + b^{(2)}$\n",
    "- $a^{(2)} = \\sigma(z^{(2)})$\n",
    "- $z^{(3)}=(W^{(3)})^{T}.a^{(2)} + b^{(3)}$\n",
    "- $\\hat{y}=a^{(3)} = \\sigma(z^{(3)})$"
   ],
   "id": "b283421c495f5c01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Biểu diễn dưới dạng matrix",
   "id": "323c3b4b1a62786c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tuy nhiên khi làm việc với dữ liệu ta cần tính dự đoán cho nhiều dữ liệu một lúc, nên gọi $X$ là ma trận $n*d$, trong đó n là số dữ liệu và $d$ là số trường trong mỗi dữ liệu, trong đó $x_{j}^{[i]}$ là giá trị trường dữ liệu thứ $j$ của dữ liệu thứ $i$. Ví dụ dataset bài trước",
   "id": "6bda65bd0c3cbccf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](image5.png)",
   "id": "4414a15c3ecd83e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- $n$ = 4\n",
    "- $d$ = 2\n",
    "- $x_{1}^{[1]}$ = 10\n",
    "- $x_{2}^{[1]}$ = 1\n",
    "- $x_{1}^{[3]}$ = 6\n",
    "- $x_{2}^{[2]}$ = 2"
   ],
   "id": "a76a9f52f8c4207f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Biểu diễn dạng ma trận của nhiều dữ liệu trong dataset:",
   "id": "33bfe7bfc6130c74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "x_{1}^{[1]}&x_{2}^{[1]}&...&x_{d}^{[1]}\\\\\n",
    "x_{1}^{[2]}&x_{2}^{[2]}&...&x_{d}^{[2]}\\\\\n",
    "...&...&...&...\\\\\n",
    "x_{1}^{[n]}&x_{2}^{[n]}&...&x_{d}^{[n]}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-(x^{[1]})^{T}-\\\\\n",
    "-(x^{[2]})^{T}-\\\\\n",
    "...\\\\\n",
    "-(x^{[n]})^{T}-\n",
    "\\end{bmatrix}\n",
    "$$"
   ],
   "id": "7195b8448ae26e55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Do $x^{[1]}$ là vector kích thước d$\\times$1 tuy nhiên ở X mỗi dữ liệu được viết theo hàng nên cần transpose $x^{[1]}$ thành kích thước 1$\\times$d, kí hiệu $-(x^{[1]})^{T}-$.",
   "id": "e34986ed34aa19a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gọi ma trận $Z^{(i)}$ kích thước $N.l^{(i)}$ trong đó $z_{j}^{(i)[k]}$ là giá trị thứ $j$ trong layer $i$ sau bước tính tổng linear của dữ liệu thứ $k$ trong dataset.",
   "id": "92c89c669f36cb1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tương tự, gọi ma trận $A^{(i)}$ kích thước $N.l^{(i)}$ trong đó $a_{j}^{(i)[k]}$ là giá trị thứ $j$ trong layer $i$ sau khi áp dụng activation function của dữ liệu thứ $k$ trong dataset.",
   "id": "1febc1f68aa25044"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "Z^{(i)} =\n",
    "\\begin{bmatrix}\n",
    "z_{1}^{(i)[1]}&z_{2}^{(i)[1]}&...&z_{l^{(i)}}^{(i)[1]}\\\\\n",
    "z_{1}^{(i)[2]}&z_{2}^{(i)[2]}&...&z_{l^{(i)}}^{(i)[2]}\\\\\n",
    "...&...&...&...\\\\\n",
    "z_{1}^{(i)[n]}&z_{2}^{(i)[n]}&...&z_{l^{(i)}}^{(i)[n]}\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-(z^{(i)[1]})^{T}-\\\\\n",
    "-(z^{(i)[2]})^{T}-\\\\\n",
    "...\\\\\n",
    "-(z^{(i)[n]})^{T}-\n",
    "\\end{bmatrix}\n",
    "$$"
   ],
   "id": "b885f9cc53eb34cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Do đó:",
   "id": "3d8b2c9aa3fd3e5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "Z^{(1)} =\n",
    "\\begin{bmatrix}\n",
    "-(z^{(1)[1]})^{T}-\\\\\n",
    "-(z^{(1)[2]})^{T}-\\\\\n",
    "...\\\\\n",
    "-(z^{(1)[n]})^{T}-\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "(x^{[1]})^{T}.w^{(1)}+(b^{(1)})^{T}\\\\\n",
    "(x^{[2]})^{T}.w^{(1)}+(b^{(1)})^{T}\\\\\n",
    "...\\\\\n",
    "(x^{[n]})^{T}.w^{(1)}+(b^{(1)})^{T}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "X.W^{(1)}+b^{(1)}\n",
    "$$"
   ],
   "id": "26660c3411b8f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$A^{(1)} = \\sigma(Z^{(1)})$$",
   "id": "53375d3ef0c90ce7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$Z^{(2)} = A^{(1)}.W^{(2)}+b^{(2)}$$",
   "id": "44b35528655921d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$A^{(2)} = \\sigma(Z^{(2)})$$",
   "id": "2a3497f8f1b1e069"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$Z^{(3)} = A^{(2)}.W^{(3)}+b^{(3)}$$",
   "id": "c4d5035916138372"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$\\hat{Y} = A^{(3)} = \\sigma(Z^{(3)})$$",
   "id": "a1c37d857b00c4dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$\\rightarrow$ vậy là có thể tính được giá trị dự đoán của nhiều dữ liệu 1 lúc dưới dạng ma trận.",
   "id": "fb6fd10dee82e9b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Giờ từ input X, ta có thể tính được giá trị dự đoán $\\hat{Y}$. Tuy nhiên việc chính cần làm là đi tìm hệ số $W$ và $b$.",
   "id": "6a1b72458959300b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Có thể nghĩ ngay tới **Gradient Descent** và việc quan trọng nhất trong thuật toán gradient descent là đi tìm đạo hàm của các hệ số đối với loss function. Và việc tính đạo hàm của các hệ số trong neural network được thực hiện bởi thuật toán **backpropagation**.",
   "id": "3813ef2c66d088c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
